Модуль 2: Основы работы с DataFrame

ТЕОРИЯ:
- DataFrame - основная структура данных в PySpark
- Создание: из коллекций, pandas DF, внешних источников
- Схема: явное задание vs inference
- Операции: выбор столбцов, фильтрация, сортировка
- Работа с NULL: isNull, drop, fill
- Форматы: CSV, JSON, Parquet (предпочтительный)

КЛЮЧЕВЫЕ МЕТОДЫ:
- read.csv(), read.json(), read.parquet() - чтение данных
- select(), filter(), where(), orderBy() - трансформации
- show(), count(), collect() - действия
- distinct(), dropDuplicates() - уникальность
- na.drop(), na.fill() - работа с NULL
- printSchema(), describe() - информация о данных

BEST PRACTICES:
- Явное задание схемы для производительности
- Использование Parquet вместо CSV при возможности
- Оптимизация количества partitions
- Избегать широких трансформаций без необходимости
- Кэширование при многократном использовании

ПРОИЗВОДИТЕЛЬНОСТЬ:
- Column pruning: выбор только нужных столбцов
- Predicate pushdown: фильтрация на источнике
- Partitioning: оптимальное разделение данных
- Serialization: использование Kryo
